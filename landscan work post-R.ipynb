{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "subfolder = 'fringe250smth7' ## areas to load\n",
    "data_path = 'C:/Users/SpiffyApple/Documents/USC/Courses/Fall2018/JorgeWinterWork/LandScan Global 2015/LandScan Global 2015/lspop2015'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I do two things: \n",
    "\n",
    "    a. I geocode Jorge's set of households to the metro areas that I found using LandScan data. \n",
    "    \n",
    "    b. I compute the exposure rate for each metropolitan area. \n",
    "    \n",
    "All data used in here come from Landscan work_ver4.R including:\n",
    "\n",
    "    - peruAreas is a point-level data of population and size of each found metro area\n",
    "    \n",
    "    - Polygon for each metro area\n",
    "    \n",
    "    - LandScan data as polygons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocode survey of households\n",
    "\n",
    "I first load the peruAreas dataset that contains the population, size, and identity for each found metropolitan area so I can do a spatial join on the landscan-found metropolitan area polygons. To geocode Jorge's surveys, I do a spatial join between the metro area polygons and the survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zone</th>\n",
       "      <th>sqrkm</th>\n",
       "      <th>pop</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>areaType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>60897.836735</td>\n",
       "      <td>-80.445783</td>\n",
       "      <td>-3.565858</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "      <td>408802.204082</td>\n",
       "      <td>-73.260778</td>\n",
       "      <td>-3.749409</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>76211.551020</td>\n",
       "      <td>-80.682040</td>\n",
       "      <td>-4.897590</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>397168.755102</td>\n",
       "      <td>-80.635240</td>\n",
       "      <td>-5.192622</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>106272.489796</td>\n",
       "      <td>-76.362008</td>\n",
       "      <td>-6.485543</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zone  sqrkm            pop          x         y areaType\n",
       "0     1     32   60897.836735 -80.445783 -3.565858     core\n",
       "1     2    103  408802.204082 -73.260778 -3.749409     core\n",
       "2     3     35   76211.551020 -80.682040 -4.897590     core\n",
       "3     4     89  397168.755102 -80.635240 -5.192622     core\n",
       "4     6     45  106272.489796 -76.362008 -6.485543     core"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load Peru Areas in their full so I can identify polygons\n",
    "df = pd.read_csv(\"/\".join([data_path, subfolder,'peruAreas_.csv']))\n",
    "df.drop(\"Unnamed: 0\",inplace=True,axis=1)\n",
    "peruAreas = gpd.GeoDataFrame(\n",
    "    df.drop(['x', 'y'], axis=1),\n",
    "    crs={'init': 'epsg:4326'},\n",
    "    geometry=[Point(xy) for xy in zip(df.x, df.y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load households\n",
    "#point = gpd.GeoDataFrame.from_file(\"/\".join([data_path,'households','households.shp'])) # or geojson etc\n",
    "\n",
    "## I load the STATA data directly instead of putting it through GIS software\n",
    "tmpPath = 'C:/Users/SpiffyApple/Documents/USC/Courses/Fall2018/JorgeWinterWork'\n",
    "hhs= pd.read_stata(\"/\".join([tmpPath, 'unique_ids.dta']))\n",
    "\n",
    "point = gpd.GeoDataFrame(\n",
    "    hhs.drop(['longitude', 'latitude'], axis=1),\n",
    "    crs={'init': 'epsg:4326'},\n",
    "    geometry=[Point(xy) for xy in zip(hhs.longitude, hhs.latitude)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: core\n",
      "Shapefiles loaded\n",
      "Loading: coreFringe\n",
      "Shapefiles loaded\n",
      "Loading: settlements\n",
      "Shapefiles loaded\n"
     ]
    }
   ],
   "source": [
    "## load identified metro areas\n",
    "polyDict = {}\n",
    "for areaType in ['core','coreFringe','settlements']:\n",
    "    print(\"Loading: %s\" %areaType)\n",
    "    polyDict[areaType] = gpd.GeoDataFrame.from_file(\"/\".join([data_path,subfolder,areaType,areaType+'.shp']))\n",
    "    print(\"Shapefiles loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['core', 'fringe', 'core&fringe', 'settlement'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peruAreas.areaType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyDict['core'] = sjoin(polyDict['core'], peruAreas[peruAreas.areaType=='core'],how='left').drop('index_right',axis=1)\n",
    "polyDict['coreFringe'] = sjoin(polyDict['coreFringe'], peruAreas[peruAreas.areaType=='core&fringe'],how='left').drop('index_right',axis=1)\n",
    "polyDict['settlements'] = sjoin(polyDict['settlements'], peruAreas[peruAreas.areaType=='settlement'],how='left').drop('index_right',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing spatial join with core\n",
      "\tSpatial join done. It took: 73.19seconds\n",
      "Performing spatial join with coreFringe\n",
      "\tSpatial join done. It took: 69.13seconds\n",
      "Performing spatial join with settlements\n",
      "\tSpatial join done. It took: 81.45seconds\n"
     ]
    }
   ],
   "source": [
    "pointDict = {}\n",
    "\n",
    "for areaType in ['core','coreFringe','settlements']:\n",
    "    print(\"Performing spatial join with %s\" %areaType)\n",
    "    start_time = time.time()\n",
    "    pointDict[areaType] = sjoin(point, polyDict[areaType], how='inner')\n",
    "    pointDict[areaType].dropna(subset=['areaType'],inplace=True) ## drop unmatched households\n",
    "    \n",
    "    print(\"\\tSpatial join done. It took: %.2fseconds\" %(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608491, 16)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "households = pd.concat(pointDict, keys=None, sort=True).drop_duplicates(subset=['field_1'])\n",
    "households.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in each area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "areaType\n",
       "core           418096\n",
       "core&fringe     52395\n",
       "settlement     138000\n",
       "Name: field_1, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of observations in each area:\")\n",
    "households.groupby('areaType').field_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "superfluousCols = ['index_right','geometry','layer','longitude','latitude']\n",
    "households.drop(superfluousCols, axis=1, inplace=True)\n",
    "households.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save households to a csv\n",
    "households.to_csv(\"/\".join([data_path, 'fringe250smth7','geoCodedhouseholds.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Exposure Rate\n",
    "\n",
    "For each square, I find its share of the population within its metro area. I then create half-mile buffers around each square, load the smoothed landscan data of Peru, and find all of the landscan squares that lie within each buffer. To conclude, I sum the population within the landscan squares for each buffer, multiply the total by the square's share, and then sum the products over each metropolitan area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: coreFringes\n",
      "Working on: settlements\n"
     ]
    }
   ],
   "source": [
    "## load shapefiles:\n",
    "squareDict = {}\n",
    "for areaType in ['coreFringesLandScan','settlementsLandScan']:\n",
    "    s = re.sub(\"LandScan\",'',areaType)\n",
    "    print(\"Working on: %s\" %s)\n",
    "    squareDict[s] = gpd.GeoDataFrame.from_file(\"/\".join([data_path,subfolder,areaType, s+'.shp']))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## geocode the squares\n",
    "areaType = 'coreFringes'\n",
    "squareDict[areaType] = sjoin(squareDict[areaType],polyDict['coreFringe'],  how='left')\n",
    "squareDict[areaType].drop(['index_right','sqrkm', 'layer_right'],axis=1,inplace=True)\n",
    "squareDict[areaType].rename(columns = {'layer_left':'sqrPop'},inplace=True)\n",
    "squareDict[areaType]['share'] = squareDict[areaType].sqrPop/squareDict[areaType]['pop']\n",
    "\n",
    "areaType = 'settlements'\n",
    "squareDict[areaType] =sjoin(squareDict[areaType],polyDict[areaType],how='left')\n",
    "squareDict[areaType].drop(['index_right','sqrkm','layer_right'],axis=1,inplace=True)\n",
    "squareDict[areaType].rename(columns = {'layer_left':'sqrPop'},inplace=True)\n",
    "squareDict[areaType]['share'] = squareDict[areaType].sqrPop/squareDict[areaType]['pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load peru landscan\n",
    "landscan = gpd.GeoDataFrame.from_file(\"/\".join([data_path,'perulandscan','peru.shp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: coreFringes\n",
      "Working on: settlements\n"
     ]
    }
   ],
   "source": [
    "## create buffers and intersect with Peru\n",
    "bufferDict = {}\n",
    "inDict = {}\n",
    "\n",
    "for areaType in ['coreFringes','settlements']:\n",
    "    print(\"Working on: %s\" %areaType)\n",
    "    bufferDict[areaType] = squareDict[areaType]\n",
    "    bufferDict[areaType]['geometry'] = squareDict[areaType].geometry.buffer(.0072)\n",
    "    inDict[areaType] = sjoin(bufferDict[areaType],landscan, how='left') # spatial join buffers and landscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute total Landscan population in the buffer\n",
    "sumDict = {}\n",
    "for areaType in ['coreFringes','settlements']:\n",
    "    sumDict[areaType] = pd.concat([squareDict[areaType], inDict[areaType].groupby(axis=0, level=0).sqrPop.agg(['sum'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename sum column to something more understandable\n",
    "sumDict['coreFringes'].rename(columns = {'sum':'totalExposure'},inplace=True)\n",
    "sumDict['settlements'].rename(columns = {'sum':'totalExposure'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on coreFringes\n",
      "working on settlements\n"
     ]
    }
   ],
   "source": [
    "## multiply shares by total population in circle\n",
    "for areaType in ['coreFringes','settlements']:\n",
    "    print(\"working on %s\" %areaType)\n",
    "    sumDict[areaType].loc[:,'exposure'] = sumDict[areaType].share*sumDict[areaType].loc[:,'totalExposure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum over squares in circles to compute exposure \n",
    "exposureDict = {}\n",
    "for areaType in ['coreFringes','settlements']:\n",
    "    exposureDict[areaType] = sumDict[areaType].groupby('zone')[['exposure','share']].sum()\n",
    "    \n",
    "## process output and save to a csv    \n",
    "exposureDF = pd.concat(exposureDict).reset_index(level=0)\n",
    "subset = (peruAreas.areaType=='core&fringe') | (peruAreas.areaType=='settlement')\n",
    "exposureDF =exposureDF.merge(peruAreas.loc[subset].drop('geometry',axis=1),left_index=True, right_on='zone')\n",
    "exposureDF.to_csv(\"/\".join([data_path, subfolder, 'exposures.csv']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
